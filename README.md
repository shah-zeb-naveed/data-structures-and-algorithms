Programming

- Asking clarifying questions
  - Input Null
  - Input data types
- Confirm solution verbally and then start coding after confirmation
- Discuss time and space complexity
- Practice more problem types than a lot of one type
- 15 minutes, medium and hard leetcode
- Technical Tips
  - Revise highlights of the Cracking the coding Interview book
  - Can use depth-first or breadth first to traverse a tree
  - Depth first is recursive to write a function that takes depth itself as input
  - Design a basic class for example, a Node Class with value, right child and left child in a binary tree
  - Think about running sums
  - Python functions argguments passed by reference
  - Maximize use of hash
  - Instead of maintaining a list, try to see if you can store running/sum and number of items in a tuple
  - When n^2 to get combinations look the first solution, see if dict can be used for linear type
  - If implementing an equation, see if problem can be simplified using equation manipulation
  - Substring is contiguous, subsequence is contiguous, combinations is not and its completely differnet
  - Look at constraints and handle all base cases
  - When encounter a situation where you have to traverse something till end, use stack
  - Recursion quite often makes tree methods quite simple
  - A class can have an object of its own class
  - When you get an indication that we need to perform addition stuff based on how much travel, this indicates a simple recursive solution
  - Try to see problems as sub-problems instead of complicated logics
  - When to use recursion and when not to?
  - If it is known that answer lies nearby the source node in graph, or to get optimal answer, BFS should be used.
    - BFS is ideal when we wanna find optimal path to take. We can make branches that can represent &quot;next potential steps&quot; and all of them would have the same cost as they all are siblings. In other words, we can label each solution and keep the label in the BFS&#39;s queue.
  - DFS if answer lies quite far away or if backtracking is needed or if entire graph has to be traversed.
    - Cycle detection in a graph
    - Solving a maze because in DFS you explore every possible path before backtracking.
    - Checking if a graph is bipartite or not
    - Finding out strongly connected components of a graph.
    - Longest path between two nodes in a graph etc.
  - Lists / Arrays:
    - See if sorting helps reduce runtime prior to logic
  -
    - Are there any cycles involved? Visualizing a complex problem and simplify for a simple yet elegant solution.
    - Often list problems involve sorting, reversal, math (equation manipulation)
  - Instead of repeatedly subtracting, take mod
  - Corner Cases: negative, null, repeated/duplicates, non-continuous, too big, too homogeneous
  - VISUALIZE. VISUALIZE. VISUALIZE. A simple solution can always come up after visualizing the problem.
  - Strings:
    - Two pointers (Expand and Contract)
  - Many times keeping counts in dictionary will help
  - Why maintain two lists when you can maintain a list of tuples (with corresponding elements) for association
  - Minor: use \_ for variables that are not required, such as loop iterator to pretend you&#39;re a pro-grammer.
  - Feel like popping multiple items from the queue just because its FIFO? Why not just slice and dice
  - Applications of algorithms like binary search don&#39;t have to be EXACT. Sometimes minor modifications might be required to get the correct answer such as modified/adding a new base case, modifying how points ideally move etc.
    - There can be ways that you first apply the general algorithm and then do some corrections to match the problem statement
  - For recursive calls, if we are &quot;searching&quot; something, we just need to do &quot;return recursive calls&quot; until we reach a base case and it returns 1.
    - There are cases where we are trying to backtrack essentially from the last failure with the help of a for loop to explore other &quot;solutions&quot; (similar to a graph structure / DFS for example). To &quot;catch&quot; a &quot;failed attempt&quot; and to continue exploring other options, catch the &quot;return False&quot; using an if condition and return True inside the if condition.
  - For recursive calls, remember to explore the option of &quot;sending&quot; original arrays, original pointers or pointers with &quot;original reference&quot; while making recursive calls. E.g. binary search.
  - Equation involved? HAVE to do some manipulation like log/anti-log and stuff.
  - Would&#39;ve been nicer if an upper bound was given so a search could be performed optimally? Try to estimate the upper bound using given info.
  - If a slightly complicated operation is actually a sub-problem that will repeat more than once, remember to implement a separate function for it.
  - Pointers suck. Might just use stack for reversing linked lists.
  - Often with lists, when we wanna find something that satisfies a certain condition, sorting will often make this simple. Also see whether starting index from beginning of the array has more chances to &quot;detect something&quot; or if you start from the last in reverse.
  - Don&#39;t try to apply algos right away. There could be simple array swapping/other operations that can be used for a simpler solution.
  - All immutables like tuples are hashable. So can use them as dict keys. Sometimes, you might wanna convert list elements to a joint string for hashing.
  - Using dict if you wanna get unique keys is okay but no need to write extra code for incrementing counts if you don&#39;t need counts
  - Can two problems have the same solution? For example, # of visible nodes from left-side is the same as returning height of the tree.
  - Safe to assume that a binary search tree is balanaced or not? Is it a binary search tree or just a binary tree?
  - Read the question properly. Does it have anything that tells you to reuse past information?
  - When you need ALL the recursive calls to maintain/update something, just keep it outside the function (global variables).
  - Dictionaries and lists are mutable objects which also imply that if they are defined outside &quot;ANY&quot; function, they can still be modified by function f or any inside f -\&gt; f(g(h(…))) so no real need to pass by reference.
  - Mutable objects have a tricky copy mechanism (a = [2], b = a, b[0] = 1 -\&gt; a = 1)
  - All About Sorting:
    - Insertion would work best if an array is already sorted / small array
      - I guess because it will quickly break the for loops
    - Bucket sort needs to have distributed data to be useful, otherwise, it&#39;s just a waste of time to achieve the same thing with extra steps.
      - More memory
      - Good for parallel
    - Counting sort won&#39;t make sense when n \&lt;\&lt;\&lt;\&lt; range
    - Quick Sort requires less space than merge sort since there is no merging. Worst case is O(n^2) but randomizing pivot leads to O(nlogn)
  - Sometimes it&#39;s just easier separating two functions, one for calling and one for recursion
  - Adjacency matrix MAY sometimes be helpful than adjacency list (0 indicates self or disconnected).
    - Mayube tree tuple of edge as a key if nothing works
  - Instead of deleting from heap, might just be better off not considering it when its time comes during popping.
  - Defining graph: If you need unique edges, defining a graph using a dict (adjacency list) will cause additional overhead.
    - Better to create a class with add\_edge method.
    - Choose format according to the problem. Do you even need a dictionary mapping or just a list of edges will work?
  - Hashing: h(large\_value) = large\_value % size\_of\_hash
  - Permutations: n! If r = n
    - ![](RackMultipart20220313-4-18zk86a_html_94c51b18d525a5a1.png)
    - ![](RackMultipart20220313-4-18zk86a_html_3776fd6110182b17.png)
  - Permutations are expensive. Avoid them at any cost:
    - Sorting
    - Windowing
    - Hashing
  - Too many if conditions is a strong indicator that a simpler solution exists
  - Plot graphs in problem similar to stocks.
  - While rotating an array (i + k) % length will give new rightful position
  - If have to shift all elements in array again and again, can we just place them in the right position using a trick? %, placeholders, cyclic dependicies
    - Does visualizing the array/linked list in circular form simply stuff?
    - Sorting algorithms that are in-place:
  - XOR is commutative and can be used to extract unique number from list
  - &quot;It&#39;s a good idea to check array sizes and use a hash map for the smaller array. It will reduce memory usage when one of the arrays is very large.&quot;
  - Maybe checking lengths of arrays might be a good idea for returning early
  - If possible, use string for string-based results instead of list for O(1) space complexity
  - It may be nice to analyze big O and preference of a solution based on how it will be used in production. If for example, there&#39;s a certain query, is it a one off thing? Will there be a lot of inserts? Deletions? Searches?
  - &quot;Having dummy nodes in any kind of linked list, singly, double or circular, makes the corner cased vanish. Thus, programming becomes a lot easier.&quot;
    - Not just corner cases. It can also help simplify code.
  - Do we explicity delete pointers in Python by setting to None or garbage collector takes care of it?

#         delete = prev.next # useless step?

#         prev.next = prev.next.next

#         delete = None # useless step?

  - If a binary tree is balanced, depth first search traversal might only be log(n) space complexity
  - Level order BFS traversal can be done using:
    - Just use a single queue if no need to process nodes at a single level
    - Curr\_level and next\_level quques OR
    - Curr\_level and n\_current\_level\_iterations
  - Here is the funny thing about BST. Inorder traversal is not a unique identifier of BST. At the same time both preorder and postorder traversals are unique identifiers of BST. From these traversals one could restore the inorder one: inorder = sorted(postorder) = sorted(preorder), and inorder + postorder or inorder + preorder are both unique identifiers of whatever binary tree.
  - the height-balanced restriction means that at each step one has to pick up the number in the middle as a root.
  - &quot;Whenever you&#39;re trying to solve an array problem in-place, always consider the possibility of iterating backwards instead of forwards through the array. It can completely change the problem, and make it a lot easier.&quot;
  - Changing Ascending to Descending may simplify a solution as well
  - Binary Search impementations can be tricky:
    - Check if you wanna include mid point for next search space or not and in what cases?
    - When does the while loop end
    - It&#39;s not necessary that the mid point will always be the answer. See if returning, high or low makes the solution cleaner.
  - And since Python 3.7, official documentation states: &quot;Dictionaries preserve insertion order.&quot;
  - Output data structures don&#39;t count in Space complexity
  - To check if prime, don&#39;t need to check divisors till n.
    - n/2 -\&gt; because there is no integer after n/2 that will multiply by 2 (the smallest partner someone can get) that will not cross n.
    - sqrt(n) -\&gt; because if see closer, e.g. 16 has 4 sqrt. 4 x 4. This means that although it may have factors above 4 (indeed it does, 8) but those factors above will ALWAYS need values less than sqrt(n). Thus, if a number is not a prime, we should be able to detect it by dividing until sqrt(n).
    - **claim # 1:** for sqrt(n) + 1 and onwards, all the composites until n will be covered by the primes below sqrt(n).
    - **claim # 2:** for a prime number p, smaller prime numbers will have marked composites until p \* p
  - In a real interview, this is a question you should ask the interviewer. Don&#39;t ever assume without asking in a real interview that the input has to be valid.
  - Odd-one out, unique, missing -\&gt; XOR
    - &amp;-ing with 1 or 0 might help
    - &quot;to retrieve the right-most bit in an integer n, one could either apply the modulo operation (i.e. n % 2) or the bit AND operation (i.e. n &amp; 1)&quot;
  - Tuples are great for keep track of some information while new information keeps coming in.
    - Also useful for designing new data structures.
  - While designing new data structures, think about
    - Tuples and storing information in them
    - Complementary data structures
  - Use tuples to store info to extract final answer instead of storing all the candidate answers:
    - For example, (window length, left, right)
  - If possible, delete useless stuff and keep the useful information in a tuple to reduce number of iterations at the cost of a single pass.
  - First of all, the requirements of  **in-place replacement and constant space**  should immediately indicate  **swapping**
  - Use default dict for concise code
  - Use tuple instead of &#39;&#39;.join while hashing
  - Use dict.values() instead of creating sublists manually
  - While working with characters, ASCII value of a character may help
  - If array is sorted or made sorted, two pointers might be helpful
  - If a loop is getting messy, try moving the chunks of code within loop up and down
  - First, make sure what do we have to output. Is it the values or the indices?
  - Use sets to avoid duplicates
  - Sorted(tuple()) might be a good idea to track results
  - Confirm that is it a binary tree or a binary search tree
  - &quot;Average case analysis makes assumptions about the input that may not be met in certain cases. Therefore, if your input is not random, in the worst case the actual performace of an algorithm may be much slower than the average case.
    - Amortized analysis makes no such assumptions, but it considers the total performance of a sequence of operations instead of just one operation.
    -
    - Dynamic array insertion provides a simple example of amortized analysis. One algorithm is to allocate a fixed size array, and as new elements are inserted, allocate a fixed size array of double the old length when necessary. In the worst case a insertion can require time proportional to the length of the entire list, so in the worst case insertion is an O(n) operation. However, you can guarantee that such a worst case is infrequent, so insertion is an O(1) operation using amortized analysis. Amortized analysis holds no matter what the input is.&quot;
    - Example:
      - &quot;When analyzing amortized time complexities, I find it easiest to reason that each node gets pushed and popped exactly once in next() when iterating over all N nodes.
That comes out to 2N \* O(1) over N calls to next(), making it O(1) on average, or O(1) amortized.&quot;
  - BT:
    - The key observation to make is: the longest path has to be between two leaf nodes.
    - In case of Binary Search Trees, only preorder or postorder traversal is sufficient to store structure information.
    - For a complete Binary Tree, level order traversal is sufficient to store the tree.
    - A full Binary is a Binary Tree where every node has either 0 or 2 children. It is easy to serialize such trees as every internal node has 2 children. We can simply store preorder traversal and store a bit with every node to indicate whether the node is an internal node or a leaf node.
    - How to store a general Binary Tree?
      - A simple solution is to store both Inorder and Preorder traversals
      - Have nulls indicated by -1 for every non-NULL node&#39;s children
  - Math:
    - Unique/Prime Factorization Theorem:
      - every integer greater than 1 can be represented uniquely as a product of prime numbers, up to the order of the factors
      - ![](RackMultipart20220313-4-18zk86a_html_75f7df9fc7a80328.png)
      - The theorem says two things about this example: first, that 1200 can be represented as a product of primes, and second, that no matter how this is done, there will always be exactly four 2s, one 3, two 5s, and no other primes in the product.
      - For example, characters can be assigned a prime number. A string will have a different product than another string but the same product as its permutations.
  - Remember: dependencies/ordering is a strong indicator of graphs and topological sort
  - Quick Sort:
    - Concept: Divide and Conquer:
      - A group of students; ask students to arrange themselves one by one.
    - The key process in quickSort is partition(). Target of partitions is, given an array and an element x of array as pivot, put x at its correct position in sorted array and put all smaller elements (smaller than x) before x, and put all greater elements (greater than x) after x. All this should be done in linear time.
  - QuickSelect:
    - It is not recursive.
    - First iteration: check all points and subsequently keep halving:
      - _N_+N/2 + n/4 … = N
  - While calculating distances of point, using squared distances might help reducing computation since its all relative.
  - Whenever k-lowest or k-highest:
    - If ordering not required, make the most out of it
    - If no, see if we can use heap
      - Heap can be further optimized by maintaining the size k. Only pushpopping when needed.
      - Even if we need min, using max can help to see if the curr max is a part of final answer or not
    - When the k-elements are not part of the actual array (such as a calculation):
      - Can use a modified binary approach, e.g. by calculating the &quot;target&quot; distance at every iteration.
        - If \&lt; k elements, all curr elements satisfiying the condition are part of the final answer
        - If \&gt; k, just reduce the domain by (high -\&gt; mid)
        - Caution: squared distances will skew datapoints resulting in uneven partitioning
    - Use QuickSelect to further optimize by doing inplace selection of k elements
  - Data Structures Udacity Course:
    - Python list is more than just a list
    - Linked Lists make inserting and deleting so easy but Arrays are good in terms of having indices.
      - Index vs Reference to next element
      - Doubly Linked List: reference to previous as well
    - While inserting in linked list, first set the next of the new element. This will reduce 1 additional line of code instead of saving tmp pointer.
    - Recent most important but still wanna keep the rest? Use stacks e.g. News Feed
    - Stack can be implemented with linked lists
    - LIFO
    - Queue: can implement using LL
      - Keep a ref to head and tail so constant time access
    - Deques have dequeuer and enqueuer at both ends
    - Priority Queue: if same priority, oldest removed
    - BS: for worst case, assume test element to be largest and always favor lower when middle is a tie in case of even numbers
    - If don&#39;t know big 0, create n vs n\_iterations table and notice patterns
    - Remember there might be a space vs time trade-off
    - Talk about all possible options in an interview
    - Improved version of bubble sort could stop doing comparisons for the very last elements
    - Average Case:
      - All possible case time (sum) / no. of cases &quot;on average&quot;
    - Space Complexity: If no extra i.e. in place, O(1) &quot;Auxillary&quot;
      - Assumes space released after every step so just look at single step
      - In recursion, you&#39;ll always be in a single branch of the call tree so calculate space complexity accordingly.
    - Merge sort, the recursive approach is called top-down.
      - Bottom down is iterative and starts from bottom
    - Please no quick sort on nearly sorted arrays
    - Improve quick sort by median
    - Space complexity of Quick Sort can be O(1) if in-place using swaps
    - Hash functions might be used for optimizing a solution
      - Commonly divide numbers by a number let&#39;s say 10
        - mod and use remainder as the index in an array
      - Why used the last digits? Because they are more random than the significant digits
      - &quot;Buckets&quot; to resolve collision
        - Still need to iterate through the collection O(n)
      - No perfect way to define a hash function
      - Compromise between hash function size: that spreads out values but doesn&#39;t use a lot of space vs one that uses less buckets but might have to do some searching within each bucket
      - Can use another hash function (double hashing)
      - Load Factor = Number of Entries / Number of Buckets
      - We can use our load factor as an indicator for when to rehash—as the load factor approaches 0, the more empty, or sparse, our hash table is.
      - On the flip side, the closer our load factor is to 1 (meaning the number of values equals the number of buckets), the better it would be for us to rehash and add more buckets. Any table with a load value greater than 1 is guaranteed to have collisions.
      - If hash function is simply mod than the divisor is simply the number of buckets
      - Dividing a bunch of multiples of 5 by another multiple of 5 will cause a lot of collisions
      - HashMaps:
        - You can store \&lt;k, v\&gt; in the bucket determined by hash(k)
      - For string keys, ASCII values to get integer value
      - Just a convention, s[0]\*31^(n-1) + s[1]\*31^(n-2) + ….
      - As long as you space, a unique hash key can be very useful for constant lookups
      - Tree is an extension of a linked list essentially (having several next elements)
        - A tree must be completely connected ALWAYS
        - No cycle (even ignoring the directions)
        - Height of leaf is 0 but depth is X (opposite relation)
      - BFS: Can do level Order traversal
      - DFS: Preorder, inorder, post-order
      - Perfect binary trees (all non-leaves have two children)
      - BST: Unbalanced is a problem (the worst case)
      - Heaps don&#39;t have to be binary
      - A binary heap must be a complete tree:
        - All levels except the last level must be full
      - Binary Tree: log(n) -\&gt; Height of the tree
      - If heap implemented through Tree like structure, it will require saving more stuff (left, right, bla bla bla)
      - The most unbalanced tree is kinda like a linked list
      - Red Back Tree Rules:

1. Each node can be either red or black
2. Each node has null black nodes (if it doesn&#39;t have 2 children)
3. If parent is red, both children are black
4. Root is black
5. All paths from root to null nodes have the same number of black nodes

      - Graphs (Networks) to be used for showing connections
        - Trees are actually types of graphs
      - Cycles allowed
      - No roots but we can have a starting point based on the problem
      - DAG: Directed, Acyclic, Graph
      - Connectivity:
        - Disconnected: if we can&#39;t reach a vertex or a group of vertices from another
        - Min number of elements to be removed for a graph to become disconnected -\&gt; shows how strong connection is
        - A weakly connected component is a maximal group of nodes that are mutually reachable by violating the edge directions.
      - Implementation:
        - OOP (Edge, Vertex objects)
        - List of edges
        - Adjacency List
        - Adjacency Matrix
        - If you&#39;re looking for node degree or no. of edges for a vertex -\&gt; Adjacency list
      - DFS: Can be implemented with stack:
        - If seen, put in stack
        - If hit a seen vertex, go back and try another one
        - Recursion is technically a stack as well
      - BFS: Queue
      - **Eulerian Path:**
        - Passes through every edge at least one once
        - Eulerian &quot;Cycle&quot;:
          - Each edge only once
          - End up at the same node
          - Possible only if all nodes have even degree or even number of edges connected to them.
        - Paths are a bit lenient:
          - Start and end nodes can have odd degree
      - **Hamiltonian Path**
        - Must go through every vertex once
        - Cycle:
          - Start and end at the same vertex
      - If you&#39;re storing a disconnected graph, not every node will be tied to an edge, so you should store a list of nodes.
      - We could probably leave it there, but storing an edge list will make our lives much easier when we&#39;re trying to print out different types of graph representations.
      - Unfortunately, having both makes insertion a bit complicated
      - Shortest Path:
        - Unweighted: BFS
        - Weighted, Undirected: Dijiktra&#39;s (Min Heap Implementation)
      - Knapsnack: Brute Force is O(2^n)
      - A problem may have both DFS and BFS solution like the count\_islands problem
      - # need a deep copy for 2d lists. can&#39;t use slicing or .copy().

- Clarifications to ask the interviewer / small talk:
  - Is the input sorted?
  - Do I need to implement classes?
  - Subsequence will be contiguous or just a subset?
  - Is it safe to assume that the input can be modified?
  - Example: &quot;What should we return when needle is an empty string? This is a great question to ask during an interview&quot;
  - What if input list has 0 or 1 elements only?
  - Can elements be negative
- Big O things:
  - Keep it simple.
  - Quote the space complexity as well. Remember recursive calls means more space
  - If a problem involves multiple queries, just quote for a single query/iteration
  - Keep variables as separate as possible
    - At the end, may simplify by combining them (approximating them)
    - For example, keep edges E and vertices V separate
- Concepts
  - Integer Divison
  - Divide and Conquer
    - Merge Sort
  - Recursion vs Iteration
  - Graph Theory
  - Tree Traversal
  - Combinatorial Problems
  - Flyod&#39;s algorithm to detect cycles in linked lists (the rabbit turtle thing). If there&#39;s a cycle, the rabbit and turtle will meet at some point other than starting point.
  - Abstract (lists, stacks) vs Concrete data types (linked lists, heap)
  - DP/memorization (Facebook said no need for ML) :D
  - Prefix Sum (I guess cumulative sums)
  - Might vary by implementation so talk about approximations
  - When things are constant, the big O is constant e.g. if we know properties of an object are 4, then looping over its properties will not be O(p) but O(1)
  -
- Data Structures
  - Hash Tables
  - Linked Lists
  - Stacks
  - Queues
  - Trees
    - Binary Tree
  - Tries
  - Graphs
  - Vectors
  - Heaps
  - Tree Algos:
    - Binary Search
    - Breadth-First
    - Depth-First
  - Sorting
    - Quick Sort
    - Merge Sort
    - Quick Select Algorithm